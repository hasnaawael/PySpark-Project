{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Spark Streaming Use Case: Analyzing Retail Data"
      ],
      "metadata": {
        "id": "YOCuGnj4fZtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Use Case Overview üìù**\n",
        " -  I'm working with a dataset comprising 243 CSV files containing retail sales data. These files are located at this [Github](https://github.com/databricks/Spark-The-Definitive-Guide/tree/master/data/retail-data).\n",
        " -My  goal is to perform real-time analysis on this data using Spark Structured Streaming. This includes processing existing data and ensuring that any new rows added to existing files or new CSV files are also processed automatically."
      ],
      "metadata": {
        "id": "FjMg9bU_hU8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1 - Build SparkSession:**"
      ],
      "metadata": {
        "id": "lTPkN4yZiAn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8I0Tak-c74I",
        "outputId": "76e5cf4a-dda3-476a-bac7-d4b98ff7abab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADcLJ_2tc5mR",
        "outputId": "c1cfaaf9-0a36-4b5b-e9c2-4580c2dc1992"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488490 sha256=4b65244883f5bdf4994790e5b77eadaa4c90356d23d2748676b15112c170bd04\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnOwgUC9jUZf",
        "outputId": "70cfca47-8195-40cd-8138-4f133ac837fd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "KTKf4dkUjYtb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = (SparkSession\n",
        ".builder\n",
        ".appName(\"project\")\n",
        ".getOrCreate())"
      ],
      "metadata": {
        "id": "ON0rtpRujYp4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Read Static Data**"
      ],
      "metadata": {
        "id": "MBhbxN91iMKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/retail-data\""
      ],
      "metadata": {
        "id": "FsiQHrK8jYk3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"recursiveFileLookup\", \"true\") \\\n",
        "    .load(data_path)"
      ],
      "metadata": {
        "id": "V1DvAILHjYb4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create or replace a temporary view for SQL operations\n",
        "df.createOrReplaceTempView(\"retail_data\")"
      ],
      "metadata": {
        "id": "_0OK5SodjYST"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Capture the schema of the DataFrame for reference\n",
        "df_schema = df.schema"
      ],
      "metadata": {
        "id": "TyXuf6MUlupJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the schema of the DataFrame to understand its structure\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79RDS_LplunH",
        "outputId": "ea65f075-c451-4876-cd77-2b42c09da512"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- InvoiceNo: string (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: timestamp (nullable = true)\n",
            " |-- UnitPrice: double (nullable = true)\n",
            " |-- CustomerID: double (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the number of files read by the DataFrame\n",
        "num_files_read = len(df.inputFiles())"
      ],
      "metadata": {
        "id": "bXV8HWiQluhd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of files read\n",
        "num_files_read"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0XodePvlue_",
        "outputId": "c687f680-742a-4287-8dde-56f22e702019"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "243"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6FZoCZ5lucF",
        "outputId": "a14d3e7c-5e01-43ff-aa6b-af7163ac4873"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   537226|    22811|SET OF 6 T-LIGHTS...|       6|2010-12-06 08:34:00|     2.95|   15987.0|United Kingdom|\n",
            "|   537226|    21713|CITRONELLA CANDLE...|       8|2010-12-06 08:34:00|      2.1|   15987.0|United Kingdom|\n",
            "|   537226|    22927|GREEN GIANT GARDE...|       2|2010-12-06 08:34:00|     5.95|   15987.0|United Kingdom|\n",
            "|   537226|    20802|SMALL GLASS SUNDA...|       6|2010-12-06 08:34:00|     1.65|   15987.0|United Kingdom|\n",
            "|   537226|    22052|VINTAGE CARAVAN G...|      25|2010-12-06 08:34:00|     0.42|   15987.0|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now let's make some transformation to  all loaded files üõ†Ô∏è**\n",
        "-  grouping the data to determine the total cost for each customer per day, ordered by total cost, and retrieving the top 10 records. We then repeat the same process with streamed data to ensure that the top 10 records are identical."
      ],
      "metadata": {
        "id": "KoHwuMNVo7m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the SQL query for transformation\n",
        "sql_query = \"\"\"\n",
        "SELECT\n",
        "    CustomerID as CustomerId,\n",
        "    sum(UnitPrice * Quantity) as total_cost,\n",
        "    window(InvoiceDate, '1 day') as date_window\n",
        "FROM\n",
        "    retail_data\n",
        "GROUP BY\n",
        "    CustomerId, window(InvoiceDate, '1 day')\n",
        "ORDER BY\n",
        "    total_cost DESC\n",
        "\"\"\"\n",
        "\n",
        "# Execute the SQL query using Spark SQL\n",
        "aggDF = spark.sql(sql_query)\n",
        "\n",
        "# Show the top 10 rows of the aggregated DataFrame\n",
        "aggDF.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8OgX4a9luXW",
        "outputId": "74d8520d-f405-4fc7-8ca9-d59e57cc4a77"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+--------------------+\n",
            "|CustomerId|        total_cost|         date_window|\n",
            "+----------+------------------+--------------------+\n",
            "|   17450.0|          71601.44|{2011-09-20 00:00...|\n",
            "|      NULL| 33521.39999999998|{2011-03-29 00:00...|\n",
            "|   18102.0|31661.540000000005|{2011-09-15 00:00...|\n",
            "|   18102.0|          25920.37|{2010-12-07 00:00...|\n",
            "|      NULL|25399.560000000012|{2010-12-10 00:00...|\n",
            "|      NULL|25371.769999999768|{2010-12-17 00:00...|\n",
            "|   12415.0| 23426.81000000001|{2011-06-15 00:00...|\n",
            "|      NULL|23395.099999999904|{2010-12-06 00:00...|\n",
            "|      NULL| 23032.59999999993|{2011-08-30 00:00...|\n",
            "|      NULL| 23021.99999999999|{2010-12-03 00:00...|\n",
            "+----------+------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3 - Streaming Data**"
      ],
      "metadata": {
        "id": "4bF8fpFUC4mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a streaming DataFrame using the specified schema from static data\n",
        "streamingdf = spark.readStream.schema(df_schema).option(\"maxFilesPerTrigger\", 1).format(\"csv\").option(\"header\", \"true\").load(\"/content/drive/MyDrive/retail-data/*.csv\")\n",
        "\n",
        "# Register the streaming DataFrame as a temporary view for SQL operations\n",
        "streamingdf.createOrReplaceTempView(\"streaming_data\")"
      ],
      "metadata": {
        "id": "wsx-wCVqluUF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the SQL query for transformation\n",
        "sql_query = \"\"\"\n",
        "SELECT\n",
        "    CustomerID as CustomerId,\n",
        "    (UnitPrice * Quantity) as total_cost,\n",
        "    InvoiceDate\n",
        "FROM\n",
        "    streaming_data\n",
        "\"\"\"\n",
        "# Register the result of the SQL query as another temporary view\n",
        "transformedDataFrame = spark.sql(sql_query)\n",
        "transformedDataFrame.createOrReplaceTempView(\"transformed_data\")"
      ],
      "metadata": {
        "id": "wHLoR0IV2twA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the final SQL query for aggregation\n",
        "final_sql_query = \"\"\"\n",
        "SELECT\n",
        "    CustomerId,\n",
        "    window(InvoiceDate, '1 day') as window,\n",
        "    SUM(total_cost) as total_cost\n",
        "FROM\n",
        "    transformed_data\n",
        "GROUP BY\n",
        "    CustomerId, window\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KvhbeGWj2tsP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the aggregated result as another temporary view\n",
        "purchaseByCustomerPerDay = spark.sql(final_sql_query)\n",
        "purchaseByCustomerPerDay.createOrReplaceTempView(\"customer_purchases\")"
      ],
      "metadata": {
        "id": "ilcQoacG2tpT"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the streaming query to write aggregated data to memory\n",
        "query = purchaseByCustomerPerDay.writeStream.format(\"memory\").queryName(\"customer_purchases\").outputMode(\"complete\").start()"
      ],
      "metadata": {
        "id": "h1fCLBss2tl0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the current status of the streaming query\n",
        "status = query.status"
      ],
      "metadata": {
        "id": "lLDSofAv2tht"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the streaming query is active\n",
        "is_active = query.isActive"
      ],
      "metadata": {
        "id": "Cxxgf3gt2taX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYrEVDZMeTKQ",
        "outputId": "73de82b3-321b-46d3-fd6a-5b29ba8fc2e7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'message': 'Processing new data',\n",
              " 'isDataAvailable': True,\n",
              " 'isTriggerActive': True}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "is_active"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oa6M1QXeTHB",
        "outputId": "9a2ff660-a540-498a-ffd1-cdf7078b7535"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the last progress report\n",
        "last_progress = query.lastProgress\n",
        "print(f\"Last progress: {last_progress}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUXffZQ1eS_1",
        "outputId": "a1a79dc6-f173-4786-d620-c08579100f6b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last progress: {'id': '41bb6ce1-19fd-46f8-a1ce-2c8f468639be', 'runId': 'da86c286-e77d-491f-b1e4-c8286ff45ff9', 'name': 'customer_purchases', 'timestamp': '2024-07-25T01:21:29.220Z', 'batchId': 9, 'numInputRows': 963, 'inputRowsPerSecond': 91.80171591992374, 'processedRowsPerSecond': 87.13355048859935, 'durationMs': {'addBatch': 10866, 'commitOffsets': 48, 'getBatch': 9, 'latestOffset': 65, 'queryPlanning': 32, 'triggerExecution': 11052, 'walCommit': 29}, 'stateOperators': [{'operatorName': 'stateStoreSave', 'numRowsTotal': 597, 'numRowsUpdated': 24, 'allUpdatesTimeMs': 824, 'numRowsRemoved': 0, 'allRemovalsTimeMs': 0, 'commitTimeMs': 13800, 'memoryUsedBytes': 245216, 'numRowsDroppedByWatermark': 0, 'numShufflePartitions': 200, 'numStateStoreInstances': 200, 'customMetrics': {'loadedMapCacheHitCount': 3600, 'loadedMapCacheMissCount': 0, 'stateOnCurrentVersionSizeBytes': 153304}}], 'sources': [{'description': 'FileStreamSource[file:/content/drive/MyDrive/retail-data/*.csv]', 'startOffset': {'logOffset': 8}, 'endOffset': {'logOffset': 9}, 'latestOffset': None, 'numInputRows': 963, 'inputRowsPerSecond': 91.80171591992374, 'processedRowsPerSecond': 87.13355048859935}], 'sink': {'description': 'MemorySink', 'numOutputRows': 597}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the query\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "iKJzKaQlenP_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the result to display the top 10 rows\n",
        "result = spark.sql(\"\"\"\n",
        "SELECT *\n",
        "FROM customer_purchases\n",
        "ORDER BY total_cost DESC\n",
        "\"\"\")\n",
        "result.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7q9vEnbenMW",
        "outputId": "359eadc1-f0bb-4bfb-fb2a-2ca8eab14167"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+------------------+\n",
            "|CustomerId|              window|        total_cost|\n",
            "+----------+--------------------+------------------+\n",
            "|      NULL|{2010-12-17 00:00...|25371.769999999768|\n",
            "|      NULL|{2010-12-14 00:00...|15929.879999999974|\n",
            "|   16029.0|{2010-12-16 00:00...| 8361.599999999999|\n",
            "|      NULL|{2011-01-21 00:00...|  8360.54000000001|\n",
            "|   14646.0|{2011-01-21 00:00...|8060.2999999999965|\n",
            "|      NULL|{2010-12-13 00:00...| 7949.909999999991|\n",
            "|   14088.0|{2011-01-21 00:00...| 7544.910000000001|\n",
            "|      NULL|{2010-12-20 00:00...| 7167.169999999999|\n",
            "|   12415.0|{2011-01-06 00:00...| 7011.379999999997|\n",
            "|      NULL|{2010-12-23 00:00...| 6412.139999999996|\n",
            "+----------+--------------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}